#!/bin/bash -ex
export DHCP_NIC=em1
export VNODE_COUNT=3

export VCOMPUTE=("${NODE_NAME}-Rinjin1","${NODE_NAME}-Rinjin2","${NODE_NAME}-Quanta")
export UCSPE=("${NODE_NAME}-UCSPE")
RUN_FIT_TEST="${RUN_FIT_TEST}"
if [ ! -z "${4}" ]; then
  RUN_FIT_TEST=$4
fi
RUN_CIT_TEST="${RUN_CIT_TEST}"
if [ ! -z "${5}" ]; then
  RUN_FIT_TEST=$5
fi
MODIFY_API_PACKAGE="${MODIFY_API_PACKAGE}"

cleanUpDockerImages(){
    images=`echo $SUDO_PASSWORD| sudo -S docker images | awk '{print $3}' | sort | uniq`
    if [ -n "$images" ]; then
        docker rmi -f $images
    fi
}

cleanUpDockerContainer(){
    local containers=$(echo $SUDO_PASSWORD |sudo -S docker ps -a -q)
    if [ "$containers" != "" ]; then
        echo "Clean Up containers : " ${containers}
        docker stop ${containers}
        docker rm  ${containers}
    fi
}

cleanUp(){
    set +e
    cleanUpDockerContainer
    cleanUpDockerImages
    echo $SUDO_PASSWORD |sudo -S service mongodb stop
    echo $SUDO_PASSWORD |sudo -S service rabbitmq-server stop
    set -e
}


apiPackageModify() {
    pushd ${WORKSPACE}/build-deps/on-http/extra
    sed -i "s/.*git symbolic-ref.*/ continue/g" make-deb.sh
    sed -i "/build-package.bash/d" make-deb.sh
    sed -i "/GITCOMMITDATE/d" make-deb.sh
    sed -i "/mkdir/d" make-deb.sh
    bash make-deb.sh
    popd
    for package in ${API_PACKAGE_LIST}; do
      sudo pip uninstall -y ${package//./-} || true
      pushd ${WORKSPACE}/build-deps/on-http/$package
        fail=true
        while $fail; do
          python setup.py install
          if [ $? -eq 0 ];then
        	  fail=false
          fi
        done
      popd
    done
}

VCOMPUTE="${VCOMPUTE}"
if [ -z "${VCOMPUTE}" ]; then
  VCOMPUTE=("jvm-Quanta_T41-1" "jvm-vRinjin-1" "jvm-vRinjin-2")
fi

TEST_GROUP="${TEST_GROUP}"
if [ -z "${TEST_GROUP}" ]; then
   TEST_GROUP="smoke-tests"
fi

execWithTimeout() {
  set +e
  # $1 command to execute
  # $2 timeout
  # $3 retries on timeout
  if [ -z "${1}" ]; then
     echo "execWithTimeout() Command not specified"
     exit 2
  fi
  cmd="/bin/sh -c \"$1\""
  #timeout default to one minute
  timeout=90
  retry=3
  result=0
  if [ ! -z "${2}" ]; then
    timeout=$2
  fi
  if [ ! -z "${3}" ]; then
    retry=$3
  fi
  echo "execWithTimeout() retry count is $retry"
  echo "execWithTimeout() timeout is set to $timeout"
  i=1
  while [[ $i -le $retry ]]
  do
    expect -c "set timeout $timeout; spawn -noecho $cmd; expect timeout { exit 1 } eof { exit 0 }"
    result=$?
    echo "execWithTimeout() exit code $result"
    if [ $result = 0 ] ; then
       break
    fi
    ((i = i + 1))
  done
  if [ $result = 1 ] ; then
       echo "execWithTimeout() command timed out $retry times after $timeout seconds"
       exit 1
  fi
  set -e
}

ucsReset() {
  cd ${WORKSPACE}/build-config/deployment/
  if [ "${USE_VCOMPUTE}" != "false" ]; then
    for i in ${UCSPE[@]}; do
      ./vm_control.sh "${ESXI_HOST},${ESXI_USER},${ESXI_PASS},reset,1,${i}_*"
    done
  fi
}




##########################  Create Nodei.yml  #####################


customize_a_infrasim_config()
{
    # make it in ${WORKSPACE}
    DIR=$1
    pushd $DIR
    wget https://raw.githubusercontent.com/InfraSIM/tools/master/packer/scripts/infrasim.yml --no-check-certificate
    sed -i 'N; s/bmc:\n    interface: ens192//' infrasim.yml
    sed -i -e '1h;2,$H;$!d;g' -e 's/        -\n            network_mode: bridge\n            network_name: br1\n            device: e1000//g' infrasim.yml
    #sed -i 's/size: 4096/size: 2048/g' infrasim.yml
    for id in $(seq 1 ${VNODE_COUNT}); do
        cp infrasim.yml  node${id}.yml
        sed -i "s/type: quanta_d51/type: ${NODE_TYPE_ARRAY[$((${id}-1))]}/"  node${id}.yml
        sed -i "s/name: default/name: node${id}\nnamespace: node${id}/" node${id}.yml
        echo -e "\nbmc:\n    interface: br0" >> node${id}.yml  # so that catalogs/bmc can have IP
    done
    popd
}

##########################  Download IVN and prepare  #####################

prepare_ivn()
{
    # to support build skupack in https://github.com/RackHD/RackHD/blob/master/test/deploy/rackhd_stack_init.py#L65
    sudo apt-get install devscripts debhelper fakeroot

    # to support the namespace setup
    sudo apt-get install -y bridge-utils openvswitch-switch
    
    cd ${WORKSPACE}
    git clone https://github.com/xiaowenjiang/ivn.git
    cd ivn
    git checkout rackhd

    customized_ivn_config
    
    
    # to ensure ESXi install successfully, change some KVM flags per @Robert
    


    if [  "$(grep nested=y /etc/modprobe.d/kvm-intel.conf)" == "" ] || [ "$(grep ept=y /etc/modprobe.d/kvm-intel.conf)" == "" ]; then
        sudo rmmod kvm_intel
        sudo sh -c "echo options kvm-intel nested=y      >> /etc/modprobe.d/kvm-intel.conf"
        sudo sh -c "echo options kvm-intel ept=y         >> /etc/modprobe.d/kvm-intel.conf"
        sudo modprobe kvm_intel
    fi
    if [  "$(grep "ignore_msrs=y" /etc/modprobe.d/kvm.conf)" == "" ]; then
        sudo rmmod kvm_intel
        sudo rmmod kvm
        sudo sh -c "echo options kvm       ignore_msrs=y >> /etc/modprobe.d/kvm.conf"
        sudo modprobe kvm
        sudo modprobe kvm_intel
    fi
    echo " KVM/KVM-Intel Flags as below :"
    cat /sys/module/kvm/parameters/ignore_msrs
    cat /sys/module/kvm_intel/parameters/nested
    cat /sys/module/kvm_intel/parameters/ept



    sudo pip install -r requirements.txt

}

##########################  IVN: create  network_configuration.yml #####################

customized_ivn_config()
{
cat << EOF > network_configuration.yml
---
# root interfaces
switches:
  br-int:                   # internal switch
    ifname: br-int
    type: static
    address: 172.31.128.1
    netmask: 255.255.252.0
    ports:
     - ifname: eth1
  br-ex:                    # external switch
    ifname: br-ex
    type: static
    address: 0.0.0.0
    metmask: 255.255.255.0
namespaces:
EOF



for id in $(seq 1 ${VNODE_COUNT}); do
    cat << EOF >> network_configuration.yml
  - name: node${id}
    interfaces:
    - ifname: einf0         # vNIC in namespace
      type: static
      address: 0.0.0.0
      bridge:
        ifname: br0
        type: dhcp

EOF
done

sed -i s/eth1/${DHCP_NIC}/ network_configuration.yml

}

##########################  Run IVN #####################

runs_ivn()
{

    ######## Create namespace and bridges ##############

    # set 2nd NIC IP to 0.0.0.0
    sudo ifconfig $DHCP_NIC  0

    if [  "$(grep "source /etc/network/interfaces.d/*" /etc/network/interfaces)" == "" ]; then
        sudo echo "source /etc/network/interfaces.d/*" >> /etc/network/interfaces
    fi

    sudo ./creator_rackhd.py delete

    sudo ./creator_rackhd.py create
    
    sudo ip netns list
}

##########################  Start InfraSIM #####################
start_infraSIM()
{
    NODE_YML_DIR=$1
    
    set +e
    # destroy previous running InfraSIM
    for id in $(seq 1 ${VNODE_COUNT}); do
        n=node${id}
        sudo infrasim node destroy ${n}
    done
    set -e
    
    #sudo infrasim init -f # Hmm... I guess we need to do it . but optional

    for id in $(seq 1 ${VNODE_COUNT}); do
        n=node${id}
        sudo infrasim config add $n    $NODE_YML_DIR/${n}.yml 
        sudo infrasim config update $n $NODE_YML_DIR/${n}.yml
        sudo infrasim node start $n
    done

    echo "Check InfraSIM running well."
    for id in $(seq 1 ${VNODE_COUNT}); do
        n=node${id}
        if [ -n "$(sudo infrasim node  status $n |grep stopped)" ] ; then
            echo "InfraSIM node $n not running successfully".
            exit -2
        fi
    done

}

nodesOff() {
  cd ${WORKSPACE}/build-config/deployment/
  if [ "${USE_VCOMPUTE}" != "false" ]; then
    for i in ${VCOMPUTE[@]}; do
      ./vm_control.sh "${ESXI_HOST},${ESXI_USER},${ESXI_PASS},power_off,1,${i}_*"
    done
  else
     ./telnet_sentry.exp ${SENTRY_HOST} ${SENTRY_USER} ${SENTRY_PASS} off ${OUTLET_NAME}
     sleep 5
  fi
}

nodesOn() {
   local infrasim_dir=${WORKSPACE}/infrasim
   start_infraSIM   $infrasim_dir
}

nodesDelete() {
    set +e
    local infrasim_dir=${WORKSPACE}/infrasim
    pushd $infrasim_dir
    # destroy previous running InfraSIM
    for id in $(seq 1 ${VNODE_COUNT}); do
        local n=node${id}
        sudo infrasim node destroy ${n}
    done
    set -e
}

nodesCreate() {
    local infrasim_dir=${WORKSPACE}/infrasim
    mkdir $infrasim_dir
    customize_a_infrasim_config  $infrasim_dir
    pushd $infrasim_dir
    prepare_ivn
    customized_ivn_config
    runs_ivn
}

vnc_record_start(){
  mkdir -p ${WORKSPACE}/build-log
  pushd ${WORKSPACE}/build-config 
  export fname_prefix="vNode"
  if [ ! -z $BUILD_ID ]; then
      export fname_prefix=${fname_prefix}_b${BUILD_ID}
  fi
  bash vnc_record.sh ${WORKSPACE}/build-log $fname_prefix &
}

vnc_record_stop(){
  #sleep 2 sec to ensure FLV finishes the disk I/O before VM destroyed
  set +e
  pkill flvrec.py
  sleep 2
  set -e
}

generateSolLog(){
  pushd ${WORKSPACE}/build-config
  bash generate-sol-log.sh > ${WORKSPACE}/sol.log &
}

generateSysLog(){
  set +e
  containerId=$( echo $SUDO_PASSWORD |sudo -S docker ps|grep "my/test" | awk '{print $1}' )
  docker exec -it $containerId dmesg > ${WORKSPACE}/build-log/dmesg.log
}

generateMongoLog(){
  set +e
  containerId=$( echo $SUDO_PASSWORD |sudo -S docker ps|grep "my/test" | awk '{print $1}' )
  echo $SUDO_PASSWORD |sudo -S docker cp $containerId:/var/log/mongodb ${WORKSPACE}/build-log
  echo $SUDO_PASSWORD |sudo -S chown -R $USER:$USER ${WORKSPACE}/build-log/mongodb
}

generateRackHDLog(){
  set +e
  containerId=$( echo $SUDO_PASSWORD |sudo -S docker ps|grep "my/test" | awk '{print $1}' )
  echo $SUDO_PASSWORD |sudo -S docker cp $containerId:/root/.pm2/logs ${WORKSPACE}/build-log
  echo $SUDO_PASSWORD |sudo -S chown -R $USER:$USER ${WORKSPACE}/build-log/logs
  mv ${WORKSPACE}/build-log/logs/*.log ${WORKSPACE}/build-log
}

setupVirtualEnv(){
  pushd ${WORKSPACE}/RackHD/test
  rm -rf .venv/on-build-config
  ./mkenv.sh on-build-config
  source myenv_on-build-config
  popd
  if [ "$MODIFY_API_PACKAGE" == true ] ; then
      apiPackageModify
  fi
}

BASE_REPO_URL="${BASE_REPO_URL}"
runTests() {
  set +e
  netstat -ntlp
  if [ "$RUN_FIT_TEST" == true ] ; then
      fitSmokeTest
  fi
  if [ "$RUN_CIT_TEST" == true ] ; then
      cp -f ${WORKSPACE}/build-config/config.ini ${WORKSPACE}/RackHD/test/config
      citSmokeTest
  fi
  set -e
}

waitForAPI() {
  timeout=0
  maxto=60
  set +e
  url=http://localhost:9090/api/2.0/nodes
  while [ ${timeout} != ${maxto} ]; do
    wget --retry-connrefused --waitretry=1 --read-timeout=20 --timeout=15 -t 1 --continue ${url}
    if [ $? = 0 ]; then 
      break
    fi
    sleep 10
    timeout=`expr ${timeout} + 1`
  done
  set -e
  if [ ${timeout} == ${maxto} ]; then
    echo "Timed out waiting for RackHD API service (duration=`expr $maxto \* 10`s)."
    exit 1
  fi
}

######################################
#    OVA POST SMOKE TEST RELATED     #
######################################
portForwarding(){
    # forward ova to localhost
    # according to vagrant/mongo/config.json and cit/fit config
    socat TCP4-LISTEN:9091,forever,reuseaddr,fork TCP4:$1:5672 &
    socat TCP4-LISTEN:9090,forever,reuseaddr,fork TCP4:$1:8080 &
    socat TCP4-LISTEN:9092,forever,reuseaddr,fork TCP4:$1:9080 &
    socat TCP4-LISTEN:9093,forever,reuseaddr,fork TCP4:$1:8443 &
    socat TCP4-LISTEN:2222,forever,reuseaddr,fork TCP4:$1:22 &
    socat TCP4-LISTEN:7080,forever,reuseaddr,fork TCP4:$1:7080 &
    socat TCP4-LISTEN:37017,forever,reuseaddr,fork TCP4:$1:27017 &
    echo "Finished ova -> localhost port forwarding"
    echo "5672->9091"
    echo "8080->9090"
    echo "9080->9092"
    echo "8443->9093"
    echo "22->2222"
    echo "7080->7080"
    echo "27017->37017"
}

fetchOVALog(){
    ansible_workspace=${WORKSPACE}/build-config/jobs/build_ova/ansible
    # fetch rackhd log
    pushd $ansible_workspace
      echo "ova-post-test ansible_host=$OVA_INTERNAL_IP ansible_user=$OVA_USER ansible_ssh_pass=$OVA_PASSWORD ansible_become_pass=$OVA_PASSWORD" > hosts
      ansible-playbook -i hosts main.yml --tags "after-test"
      mkdir -p ${WORKSPACE}/build-log
      for log in `ls *.log | xargs` ; do
        cp $log ${WORKSPACE}/build-log
      done
    popd
}

prepareDockerImage(){
    pushd $WORKSPACE
    echo "[Info] Downloading rackhd_pipeline_docker image"

    # -c  resume getting a partially-downloaded file.
    # -nv reduce the verbose output
    # -t 5  the retry counter
    wget -c -t 5 -nv --no-check-certificate http://rackhdci.lss.emc.com/job/Archiving/job/BuildDockerImage/lastSuccessfulBuild/artifact/rackhd_pipeline_docker.tar

    if [ $? -ne 0 ]; then
        echo "[Error]: wget download failed: ${remote_file}"
        exit 2
    else
        echo "[Info] wget download successfully ${remote_file}"
    fi
    echo $SUDO_PASSWORD |sudo -S docker load -i rackhd_pipeline_docker.tar
    popd
}

dockerUp(){
    prepareDockerImage
    cp -r ${WORKSPACE}/build-deps ${WORKSPACE}/build-config/jobs/pr_gate/docker
    pushd ${WORKSPACE}/build-config/jobs/pr_gate/docker
    #cp -r ${WORKSPACE}/build-config/jobs/pr_gate/docker/* .
    echo $SUDO_PASSWORD |sudo -S docker build -t my/test .
    echo $SUDO_PASSWORD |sudo -S docker run --net=host -d -t my/test
    popd
}

setupTestsConfig(){
    if [ "$RUN_FIT_TEST" == true ] ; then
        cp ${WORKSPACE}/build-config/jobs/pr_gate/test_config/FIT/*.* ${WORKSPACE}/RackHD/test/config
    fi
    if [ "$RUN_CIT_TEST" == true ] ; then
        cp ${WORKSPACE}/build-config/jobs/pr_gate/test_config/CIT/config.ini ${WORKSPACE}/RackHD/test/config/
    fi
    pushd ${WORKSPACE}/RackHD/test/config
    sed -i "s/RACKHD_SSH_USER = vagrant/RACKHD_SSH_USER = ${SUDO_USER}/g" config.ini
    sed -i "s/RACKHD_SSH_PASSWORD = vagrant/RACKHD_SSH_PASSWORD = $SUDO_PASSWORD/g" config.ini
    sed -i "s/\"username\": \"vagrant\"/\"username\": \"${SUDO_USER}\"/g" credentials_default.json
    sed -i "s/\"password\": \"vagrant\"/\"password\": \"$SUDO_PASSWORD\"/g" credentials_default.json
    popd
}

runTestsForDocker() {
    set +e
    netstat -ntlp
    setupTestsConfig
    if [ "$RUN_FIT_TEST" == true ] ; then
        fitSmokeTest
    fi
    if [ "$RUN_CIT_TEST" == true ] ; then
        citSmokeTest
    fi
    set -e
}

collectTestReport()
{
    pushd ${WORKSPACE}/RackHD/test
    mkdir -p ${WORKSPACE}/xunit-reports
    cp *.xml ${WORKSPACE}/xunit-reports
    popd
}

fitSmokeTest()
{
    set +e
    echo "########### Run FIT Stack Init #############"
    pushd ${WORKSPACE}/RackHD/test
    #TODO Parameterize FIT args
    echo ${TEST_STACK}
    tstack="${TEST_STACK}"
    read hwarray<<<"${EXTRA_HW}"
    for item in $hwarray; do
        if [ "${item}" == "ucs" ] && [ "${TEST_STACK}" == "-stack vagrant" ]; then
            tstack="-stack vagrant_ucs"
        fi
    done
    echo ${tstack}
    #python run_tests.py -test deploy/rackhd_stack_init.py -stack local_run -numvms 3 -rackhd_host localhost -port 9090 -v 4 -xunit
    python run_tests.py -test deploy/rackhd_stack_init.py ${tstack} -xunit
    if [ $? -ne 0 ]; then
        echo "Test FIT failed running deploy/rackhd_stack_init.py"
        collectTestReport
        exit 1
    fi
    echo "########### Run FIT Smoke Test #############"
    #python run_tests.py ${TEST_GROUP} -stack local_run -numvms 3 -rackhd_host localhost -port 9090 -v 4 -xunit
    python run_tests.py ${TEST_GROUP} ${tstack} -v 4 -xunit
    if [ $? -ne 0 ]; then
        echo "Test FIT failed running smoke test"
        collectTestReport
        exit 1
    fi
    collectTestReport
    popd
    set -e
}

citSmokeTest(){
    echo "########### Run CIT Smoke Test #############"
    read array<<<"${TEST_GROUP}"
    args=()
    group=" --group="
    for item in $array; do
        args+="${group}${item}"
    done
    pushd ${WORKSPACE}/RackHD/test
    RACKHD_BASE_REPO_URL=${BASE_REPO_URL} RACKHD_TEST_LOGLVL=INFO \
    python run.py ${args} --with-xunit
    collectTestReport
    popd
}

exportLog(){
    vnc_record_stop
    generateRackHDLog
    generateMongoLog   
}
######################################
#  OVA POST SMOKE TEST RELATED END   #
######################################

if [ "$RUN_CIT_TEST" == true ] || [ "$RUN_FIT_TEST" == true ] ; then
  if [ "$TEST_TYPE" == "ova" ]; then
    # based on the assumption that in the same folder, the VMs has been exist normally. so don't destroy VM here.
    
    nodesCreate
    
    # Prepare RackHD
    # Forward local host port to ova
    portForwarding ${OVA_INTERNAL_IP}

    # We setup the virtual-environment here, since once we
    # call "nodesOn", it's a race to get to the first test
    # before the nodes get booted far enough to start being
    # seen by RackHD. Logically, it would be better IN runTests.
    # We do it between the vagrant and waitForAPI to use the
    # time to make the env instead of doing sleeps...
    setupVirtualEnv
    waitForAPI
    nodesOn &
    # Doesn't support ova smoke test now
    # generateSolLog
    # Run tests
    runTests
    # exit venv
    deactivate

    # Remedial work
    # Specific remedial work
    fetchOVALog

    # Clean Up below

    #shutdown vagrant box and delete all resource (like removing vm disk files in "~/VirtualBox VMs/")
    #cleanupVMs
    #nodesDelete
  elif [ "$TEST_TYPE" == "docker" ]; then
    # based on the assumption that in the same folder, the VMs has been exist normally. so don't destroy VM here.
    
    nodesCreate
    
    # Prepare RackHD
    # Forward local host port to ova
    portForwarding localhost

    # We setup the virtual-environment here, since once we
    # call "nodesOn", it's a race to get to the first test
    # before the nodes get booted far enough to start being
    # seen by RackHD. Logically, it would be better IN runTests.
    # We do it between the vagrant and waitForAPI to use the
    # time to make the env instead of doing sleeps...
    setupVirtualEnv
    waitForAPI
    nodesOn &
    # Doesn't support ova smoke test now
    # generateSolLog
    # Run tests
    runTests
    # exit venv
    deactivate

    # Clean Up below

    #shutdown vagrant box and delete all resource (like removing vm disk files in "~/VirtualBox VMs/")
    #cleanupVMs
    #nodesDelete
  else
    # rese the UCSPE emulators 
    ucsReset
    cleanUp
    # register the signal handler to export log
    trap exportLog SIGINT SIGTERM SIGKILL EXIT
    nodesCreate
    dockerUp
    # Setup the virtual-environment
    setupVirtualEnv
    waitForAPI
    nodesOn

    generateSolLog
    vnc_record_start
    # Run tests
    runTestsForDocker
  fi

fi
